{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Cxed13GY6td"
   },
   "source": [
    "# LCP mod. B – Exercise n. 1: Gradient Descent & DNNs\n",
    "\n",
    "### List of students\n",
    "\n",
    "```\n",
    "Davide Bacilieri 2089214\n",
    "Lorenzo Barbiero 2082142\n",
    "Guglielmo Bordin 2088622\n",
    "Alessio Pitteri  2090594\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVZHiLAMZ39_"
   },
   "source": [
    "## Step 0. Importing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JuHqKlHoP-vF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import keras as kr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import keras.optimizers as opt\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('image', cmap='copper')\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%run nonlinear_function.py\n",
    "\n",
    "# set tensorflow seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "import tensorflow.random as tfr\n",
    "tfr.set_seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzQivdXAZ3BK"
   },
   "source": [
    "## Step 1. Loading and Processing the Data\n",
    "We’ll be doing cross validation later, so we’re not splitting the dataset in training and test sets until after we’ve chosen the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ra3qAWGQIAV",
    "outputId": "d38488d7-6640-44ce-c358-cda0ede2dd72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4000\n"
     ]
    }
   ],
   "source": [
    "TYPE = 1\n",
    "x = np.loadtxt(filename('data', TYPE), delimiter=' ')\n",
    "y = np.loadtxt(filename('labels', TYPE), delimiter=' ')\n",
    "N = len(x)     # number of samples\n",
    "L = len(x[0])  # dimension of each sample\n",
    "\n",
    "# rescaling of the data (as done in class)\n",
    "x /= 50\n",
    "\n",
    "print(f'Number of samples: {N}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz0fC1xBbBDH"
   },
   "source": [
    "## Step 2. Definition of the Model\n",
    "Here we build the architecture: we’ve chosen to build a network with four hidden layers of the same size followed by a single dropout layer. The output layer is a single neuron with sigmoid activation, to map the output to the range $(0, 1)$.\n",
    "\n",
    "The module construction function requires three inputs: the dropout percentage for the second-to-last layer, the optimizer name, and the size of each hidden layer. We’ll optimize all of these parameters through a grid search later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZxT8MXN5eTC5"
   },
   "outputs": [],
   "source": [
    "# for some reason, GridSearchCV needs a default optimizer value here\n",
    "def compile_model(dropout_rate, hlayer_size, optimizer=opt.Adam):\n",
    "    # build DNN layer by layer\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Dense(L, input_shape=(L, ), activation='relu'))\n",
    "    # 4 hidden layers\n",
    "    model.add(Dense(hlayer_size, activation='relu'))\n",
    "    model.add(Dense(hlayer_size, activation='relu'))\n",
    "    model.add(Dense(hlayer_size, activation='relu'))\n",
    "    model.add(Dense(hlayer_size, activation='relu'))\n",
    "    # dropout layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # last single-neuron layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss=kr.losses.binary_crossentropy,\n",
    "                  optimizer=optimizer(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCVrisQZjIqu"
   },
   "source": [
    "## Step 3. Grid Search Optimization\n",
    "Here we’ll perform a grid search optimization with cross-validation, thanks to the function `GridSearchCV` from `scikit-learn`. As anticipated, we’ll search for the best combination of the optimization algorithm, the size of the hidden layers and the dropout rate for the second-to-last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u9EdtqLfjDbK"
   },
   "outputs": [],
   "source": [
    "optalg_list = [opt.Adam, opt.RMSprop, opt.Nadam, opt.Adamax]\n",
    "hsize_list = [10, 15, 20, 25]\n",
    "dout_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optalg_list,\n",
    "                  hlayer_size=hsize_list,\n",
    "                  dropout_rate=dout_list)\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(model=compile_model,\n",
    "                                   optimizer=opt.Adam,\n",
    "                                   dropout_rate=0.01,\n",
    "                                   hlayer_size=20,\n",
    "                                   epochs=400,\n",
    "                                   batch_size=50,\n",
    "                                   verbose=0)\n",
    "# call scikit grid search module\n",
    "# adjust n_jobs to number of cores on your machine\n",
    "grid = GridSearchCV(estimator=model_gridsearch,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=2, # 2 for colab\n",
    "                    cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0KXYuXt0MZx"
   },
   "outputs": [],
   "source": [
    "grid_result = grid.fit(x, y)\n",
    "\n",
    "# summarize results\n",
    "print(f'Best:\\n {grid_result.best_score_}')\n",
    "print(f'using:\\n {grid_result.best_params_}\\n')\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, par in zip(means, stds, params):\n",
    "    print(f'Score {mean} ± {stdev} with: {par}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l53ZzEdGPV2g"
   },
   "source": [
    "## Step 4. Visualization of the Grid Search Results\n",
    "Here we’ll plot the mean cross-validation score of the various combination of parameters analysed in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QW3Q7lXw21Xr",
    "outputId": "3b42f895-af58-44f2-c37d-50eaec397326"
   },
   "outputs": [],
   "source": [
    "glob = [(m, p) for m, p in zip(means, params)]\n",
    "# sort by optimizer name, alphabetically\n",
    "glob = sorted(glob, key=lambda i: str(i[1]['optimizer']))\n",
    "\n",
    "n_hsize = len(hsize_list)\n",
    "n_dout = len(dout_list)\n",
    "opt_chunk = n_hsize * n_dout\n",
    "\n",
    "# human readable names for the algorithms\n",
    "opt_names = ['Adam', 'Adamax', 'Nadam', 'RMSprop']\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(optalg_list), nrows=1, figsize=(16, 7))\n",
    "for i_op in range(len(optalg_list)):\n",
    "    # all data from the i_op-th optimizer\n",
    "    op = glob[i_op * opt_chunk:(i_op + 1) * opt_chunk]\n",
    "    # dataframe with different learning rates in the columns\n",
    "    # and different dropouts in the rows\n",
    "    d = [[op[i + n_hsize * j][0]\n",
    "          for i in range(n_hsize)]\n",
    "         for j in range(n_dout)]\n",
    "    df = pd.DataFrame(d, index=dout_list, columns=hsize_list)\n",
    "    sns.heatmap(df, annot=True, cmap='Blues',\n",
    "               # common colorbar displayed in the last plot\n",
    "               ax=ax[i_op], vmin=0.8, vmax=1, cbar=i_op==3)\n",
    "    # set optimizer name as title\n",
    "    ax[i_op].set_title(opt_names[i_op])\n",
    "\n",
    "# common labels\n",
    "fig.supxlabel('Hidden layers size')\n",
    "fig.supylabel('Dropout value')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Increasing and Reducing the Amount of Data\n",
    "Here we’ll investigate what happens when we change the amount of input data: we’ll start from the previous dataset, and we’ll train our best model on different percentanges of it.\n",
    "\n",
    "First, we’ll set the hyper-parameters to the best values that we found with the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_opt = grid_result.best_params_['optimizer']\n",
    "best_hsize = grid_result.best_params_['hlayer_size']\n",
    "best_dout = grid_result.best_params_['dropout_rate']\n",
    "print(best_opt, best_hsize, best_dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll fix the validation set to 20% of the total training data. Then, we’ll gradually decrease the training set size from the starting 80% and plot the results each time. We’ll display a plot for the training and validation accuracy through the epochs, a similar plot for the cross-entropy loss, and a scatter plot with the color-coded prediction of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first training/validation partition\n",
    "(x, y) = shuffle(x, y, random_state=12345)\n",
    "perc_train = 0.8\n",
    "N_train = int(perc_train * N)\n",
    "\n",
    "(x_train, y_train) = (x[0:N_train], y[0:N_train])\n",
    "(x_valid, y_valid) = (x[N_train:], y[N_train:])\n",
    "# data was already rescaled\n",
    "\n",
    "# plot of the first sets\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
    "ax[0].set_title('Training set (80%)')\n",
    "ax[1].scatter(x_valid[:, 0], x_valid[:, 1], c=y_valid)\n",
    "ax[1].set_title('Validation set (20%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, AX = plt.subplots(5, 3, figsize=(16, 30))\n",
    "\n",
    "# fit parameters\n",
    "nepochs = 400\n",
    "bsize = 50\n",
    "\n",
    "# lists to save the losses and accuracies for later plotting\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "row = 0\n",
    "percentages = range(100, 0, -5)\n",
    "nrows = len(percentages)\n",
    "\n",
    "g=0\n",
    "for n in percentages:\n",
    "    print(n, '% of 1st training set')\n",
    "    # first shuffle\n",
    "    (x_train, y_train) = shuffle(x_train, y_train, random_state=1234567)\n",
    "    # then cut\n",
    "    perc_train = int(N_train * n / 100)\n",
    "    (x_train, y_train) = (x_train[0:perc_train], y_train[0:perc_train])\n",
    "\n",
    "    # function to build the model defined before\n",
    "    model = compile_model(best_dout, best_hsize, best_opt)\n",
    "\n",
    "    fit = model.fit(x_train, y_train,\n",
    "                    epochs=nepochs, batch_size=bsize,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=0)\n",
    "\n",
    "    results = model.evaluate(x_valid, y_valid)\n",
    "    predictions = model.predict(x_valid)\n",
    "\n",
    "    loss, acc = results\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)\n",
    "\n",
    "    \n",
    "    if (row % 5 == 0 or row == (nrows-1) ):\n",
    "        # accuracy plot\n",
    "        ax = AX[g][0]\n",
    "        ax.plot(fit.history['accuracy'], label='Train', c='b')\n",
    "        ax.plot(fit.history['val_accuracy'], label='Valid.', c='r')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "\n",
    "        # loss plot\n",
    "        ax = AX[g][1]\n",
    "        ax.plot(fit.history['loss'], label='Train', c='b')\n",
    "        ax.plot(fit.history['val_loss'], label='Valid.', c='r')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "\n",
    "        # prediction plot\n",
    "        ax = AX[g][2]\n",
    "        ax.scatter(x_valid[:, 0], x_valid[:, 1], c=predictions)\n",
    "        ax.set_title(f'{n}% of 1st training set')\n",
    "        g += 1\n",
    "        \n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a plot of the accuracies and the losses as a function of the training set size (i.e., percentage of the first training set size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(percentages, losses, label='Loss')\n",
    "plt.scatter(percentages, accs, label='Accuracy')\n",
    "plt.xlabel('% of the 1st training set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Data Augmentation\n",
    "In this section we’ll implement an *augmentation* of the training data, i.e. we’ll generate new samples by applying random transformations to the starting data. Of course, this is not at all the same as getting actual new data, so we’re investigating whether this fictitious enlargement of the training set does any good to the training.\n",
    "\n",
    "First, we’ll report the reasoning behind our choice of the data augmentation method. To illustrate the available options and their potential issues (especially in the boundaries of the training space, which were given special attention), we’ll create a dummy dataset. Then, we’ll go over the three options we came up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "data = 2 * npr.rand(2, n) - 1\n",
    "plt.scatter(data[1], data[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First method: simply adding random noise\n",
    "This is easy to implement, but the training region ends up being bigger than the validation (and original data) region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "data_aug = data + alpha * (2 * npr.rand(2, n) - 1)\n",
    "plt.scatter(data[1], data[0], label='Starting data')\n",
    "plt.scatter(data_aug[1], data_aug[0], label='Augmented data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second method: adding random noise and truncate to the original boundaries\n",
    "This is also easy to implement, it’s the same as the first with the addition of a mask. However, the augmented data density will be lower in the boundary region, due to the possible rejection of newly generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = data + alpha * (2 * npr.rand(2, n) - 1)\n",
    "\n",
    "# boolean mask to remove points outside of [-1, 1] in x and y coordinates\n",
    "mask = (np.abs(data_aug[0]) <= 1) & (np.abs(data_aug[1]) <= 1)\n",
    "data_aug = data_aug[:, mask]\n",
    "\n",
    "plt.scatter(data[1], data[0], label='Starting data')\n",
    "plt.scatter(data_aug[1], data_aug[0], label='Augmented data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third method: adding random noise and treat the boundaries as reflective walls\n",
    "For example, if an `x` is generated as `1.03` it will be transformed to `x = 0.97`, as per a reflection. This is slightly more difficult to implement, but it should solve both of the previous issues, since samples outside the boundary are put back in the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = data + alpha * (2 * npr.rand(2, n) - 1)\n",
    "\n",
    "# reflect points outside of [-1, 1] in x and y coordinates\n",
    "data_aug[0] = np.where(np.abs(data_aug[0]) <= 1,\n",
    "                       data_aug[0],\n",
    "                       np.sign(data_aug[0]) * (2 - np.abs(data_aug[0])))\n",
    "data_aug[1] = np.where(np.abs(data_aug[1]) <= 1,\n",
    "                       data_aug[1],\n",
    "                       np.sign(data_aug[1]) * (2 - np.abs(data_aug[1])))\n",
    "\n",
    "plt.scatter(data[1], data[0], label='Starting data')\n",
    "plt.scatter(data_aug[1], data_aug[0], label='Augmented data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s choose the third method and proceed with the augmentation of the actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train = 0.8\n",
    "N_train = int(perc_train * N)\n",
    "\n",
    "(x, y) = shuffle(x, y, random_state=12345)\n",
    "(x_train, y_train) = (x[0:N_train], y[0:N_train])\n",
    "(x_valid, y_valid) = (x[N_train:], y[N_train:])\n",
    "print(f'Number of training samples: {N_train}')\n",
    "print(f'Number of validation samples: {N - N_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of copies of the data to augment\n",
    "N_aug = 2\n",
    "# a reasonable choice of the alpha parameter (after some tests)\n",
    "alpha = 0.03\n",
    "\n",
    "x_aug = x_train\n",
    "y_aug = y_train\n",
    "for i in range(0, N_aug):\n",
    "    # generate\n",
    "    data_aug_x = x_train + alpha * (2 * npr.rand(N_train, 2) - 1)\n",
    "    # reflect across the boundaries\n",
    "    data_aug_x[:, 0] = np.where(np.abs(data_aug_x[:, 0]) <= 1,\n",
    "                                data_aug_x[:, 0],\n",
    "                                np.sign(data_aug_x[:, 0])\n",
    "                                        * (2 - np.abs(data_aug_x[:, 0])))\n",
    "    data_aug_x[:, 1] = np.where(np.abs(data_aug_x[:, 1]) <= 1,\n",
    "                                data_aug_x[:, 1],\n",
    "                                np.sign(data_aug_x[:, 1])\n",
    "                                        * (2 - np.abs(data_aug_x[:, 1])))\n",
    "    # join to the original data\n",
    "    x_aug = np.concatenate((x_aug, data_aug_x), axis=0)\n",
    "    y_aug = np.concatenate((y_aug, y_train))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_aug[:, 0], x_aug[:, 1], c=y_aug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the model with the best architecture…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model(dropout_rate=best_dout,\n",
    "                      hlayer_size=best_hsize,\n",
    "                      optimizer=best_opt)\n",
    "\n",
    "fit = model.fit(x_aug, y_aug,\n",
    "                epochs=1000, batch_size=bsize,\n",
    "                validation_data=(x_valid, y_valid),\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "… and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, AX = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax = AX[0]\n",
    "ax.plot(fit.history['accuracy'], label='Train', c='b')\n",
    "ax.plot(fit.history['val_accuracy'], label='Valid.', c='r')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "ax = AX[1]\n",
    "ax.plot(fit.history['loss'], label='Train', c='b')\n",
    "ax.plot(fit.history['val_loss'], label='Valid.', c='r')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ll plot the data with the prediction, highlighting the region where the predicted label is bigger than $1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = 2\n",
    "X1 = np.arange(-50, 50 + dX, dX)\n",
    "LG = len(X1)\n",
    "\n",
    "grid = np.zeros((LG * LG, 2))\n",
    "k = 0\n",
    "\n",
    "for i in range(LG):\n",
    "    for j in range(LG):\n",
    "        grid[k, :] = (X1[j], X1[i])\n",
    "        k += 1\n",
    "grid_r = grid / 50\n",
    "\n",
    "pred = model.predict(grid_r)\n",
    "\n",
    "def boundaries():  \n",
    "    x1 = -25\n",
    "    y1 = -35\n",
    "    c = '#AAAAFF'\n",
    "    a = 0.5\n",
    "    lw = 5\n",
    "    ax.plot((50, -20), (-20, 50), c=c, alpha=a, lw=lw)\n",
    "    ax.plot((50, 0), (0, 50), c=c, alpha=a, lw=lw)\n",
    "    ax.plot((x1, 50), (y1, y1), c=c, alpha=a, lw=lw)\n",
    "    ax.plot((x1, x1), (y1, 50),c=c, alpha=a, lw=lw)\n",
    "\n",
    "fig, AX = plt.subplots(1, 3, figsize =(16, 5))\n",
    "\n",
    "ax = AX[0]\n",
    "ax.scatter(x[:, 0] * 50, x[:, 1] * 50, c=y)\n",
    "boundaries()\n",
    "ax.set_title('Data')\n",
    "\n",
    "ax = AX[1]\n",
    "ax.scatter(grid[:, 0], grid[:, 1], c=pred)\n",
    "boundaries()\n",
    "ax.set_title('Prediction $\\\\hat y$')\n",
    "\n",
    "ax = AX[2]\n",
    "W1 = np.where(pred > 0.5)[0] \n",
    "ax.scatter(grid[:, 0], grid[:, 1], c='#000000')\n",
    "ax.scatter(grid[W1, 0], grid[W1, 1], c='#ffc77f')\n",
    "boundaries()\n",
    "ax.set_title('Where $\\\\hat y > 1/2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with various amounts of noise ($0.01 < \\alpha < 0.05$) shows that, despite satisfying performances on the validation set, the training on augmented data can never be as stable, reliable and efficient as the training on just real data. This is reasonable, since random noise is, by definition, not learnable; so, at best we can achieve the same result as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
